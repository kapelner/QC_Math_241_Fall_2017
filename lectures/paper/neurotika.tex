\documentclass[12pt]{article}

\include{preamble}

%\title{You Can't Judge a Book by its Author --- or Can you?}
\title{More than Maleness or Femaleness Should be Considered In Social Science}
\title{Having an Ambiguous Gender}

\author[1]{Dana Beth Weinberg\thanks{Electronic address: \texttt{Dana.Weinberg@qc.cuny.edu}; Prinicipal Corresponding author}}
\author[2]{Adam Kapelner\thanks{Electronic address: \texttt{Kapelner@qc.cuny.edu}; Corresponding author}}

\affil[1]{Dept. of Sociology,  Queens College, The City University of New York}
\affil[2]{Dept. of Mathematics,  Queens College, The City University of New York}

\begin{document}
\maketitle

\begin{abstract}
When analyzing survey and experimental data, social scientists generally consider only gender being binary: male and female. An ambiguous gender is generally treated as a nuisance which needs to be imputed to either male or female or those rows omitted from the analysis. We demonstrate here that ambiguous or \qu{androgynous} gender is an important third category that should be considered for future research. We first observe in a dataset of published book prices that authors whose names are of androgynous gender (first name being initials) command higher book prices. We could not rule out omitted variable bias when assessing if this effect was causal. So we embarked on a large $n \approx 2500$ randomized natural field experiment using Amazon's Mechanical Turk (we discuss this methodology herein in detail). We provided cover images, titles and description blurbs from published e-books in two genres while manipulating the author's gender: male, female and androgynous. We asked participants to assess the book's value and found that people would indeed pay more (\$0.25) for an androgynously-authored book. We connect this result to the psychology of suprise literature and discuss implications.
\end{abstract}
\vspace{8cm}\pagebreak

\section{Introduction and Motivation}\label{sec:intro}

Consider the following scenario facing us: we were analyzing book publishing data for gender bias from xx containing every book published in the past yy years with information about price, author(s), year, genre, etc. In order to assess gender bias, we first need the gender of the author. So we use a lookup table of common male and female names and match based on a threshold score. We code multiple authors as \qu{ineligible} and we code only initials as \qu{initials} and everything else as \qu{A}.

Our gender variable looks like this:


\begin{verbatim}
M   F   A       ineligible    initials
833 848 863     1111          333
\end{verbatim}

To investigate bias, we would regress a dummy for male (or female) on price and attempt to control for all omitted variable bias using other covariates in the dataset (genre, year, publisher information, etc). In this case, we would drop all non-male or non-female 

where 


%Work by women not as valued (vita stuff).
%
%Is this a response to market? LEt's see on the consumer side - does that lead to differences in willingness to pay and other things?
%
%Then the behavior we're seeing from publishers may be misplaced.
%
%We perform the first natural field experiment (Harrison and List, 2004) in a real effort task that manipulates gender. This method overcomes a number of shortcomings of the previous literature, including: interview bias, omitted variable bias (which can confuse causation with correlation), and concerns of external validity.
%
%We recruited workers from the United Statesfrom Amazon’s Mechanical Turk (MTurk), an online labor market
%where people around the world complete short, “one-off” tasks for pay. The MTurk environment is a spot market for labor
%characterized by relative anonymity and a lack of strong reputational mechanisms. As a result, it is well-suited for an
%experiment involving the meaningfulness of a task since the variation we introduce regarding a task’s meaningfulness is
%less affected by desires to exhibit pro-social behavior or an anticipation of future work (career concerns). We ensured that
%our task appeared like any other task in the marketplace and was comparable in terms of difficulty, duration, and wage.
%Our study is representative of the kinds of natural field experiments for which MTurk is particularly suited (see Section 2)
%
%We contribute to the literature on ...
%
%conclusion: we haven't found any support for what publishers are doing...

The rest of this paper is organized as follows. We examine the background literature in Section~\ref{sec:background}. We describe experimentation using MTurk in Section~\ref{sec:mturk}. Our experimental design is found in Section~\ref{sec:design}. We discuss our results in Section~\ref{sec:results} and conclude and offer future directions in Section~\ref{sec:discussion}.

\section{Background Research}\label{sec:background}

\citet{Foschi1996}
\citet{Dixon2015}
\citet{Cheng2011}
\citet{Bortolussi2010}
\citet{Borsuk2009}
\citet{Bobbit-Zeher2011}
\citet{Barnes2015}
\citet{Alexander2014}
\citet{Alexander1993}
\citet{Ridgeway1997}
\citet{Yampbell2005}
\citet{VanDijk2014}
\citet{VandenBrink2012}
\citet{Uscinski2011}
\citet{Tregenza2002}
\citet{Top1991}
\citet{Paludi1983}
\citet{Lloyd1990}
\citet{Lena2014}
\citet{Leemans1992}
\citet{Johnston2014}
\citet{Haswell1996}
\citet{Gorman2005}
\citet{Fulton2012}
\citet{Foschi2000}





\section{Experimentation on MTurk}\label{sec:mturk}

Internal Validity

Yes

External Validity

Yes... external validity is only a problem

MTurk’s potential as a platform for field experimentation using the framework proposed in Levitt and List (2007, 2009) is explored in Chandler and Kapelner

Amazon’s Mechanical Turk \citep{MTurkWebsite} is the largest online, task-based labor market and is used by hundreds of thousands \citep{Shank2015}.
of people worldwide. Individuals and companies can post tasks (known as Human Intelligence Tasks, or “HITs”) and have
them completed by an on-demand labor force. Typical tasks include image labeling, audio transcription, and basic Internet

research. Academics also use MTurk to outsource low-skilled resource tasks such as identifying linguistic patterns in text
(Sprouse, 2011) and labeling medical images (Holmes and Kapelner, 2010). The image labeling system from the latter study,
known as “DistributeEyes,” was originally used by breast cancer researchers and was modified for our experiment.
Beyond simply using MTurk as a source of labor, academics have also began using MTurk as a way to conduct online
experiments. The remainder of the section highlights some of the ways this subject pool is used and places special emphasis
on the suitability of the environment for natural field experiments in economics.


\citet{Berinsky2012}
\citet{Buhrmester2011}
\citet{Henrich2010}
\citet{Paolacci2010}
\citet{Gneezy2006}
\citet{Levitt2007}
\citet{Levitt2009}
\citet{Kapelner2010}
\citet{Chandler2013}
\citet{Horton2010}
\citet{Harrison2004}

\section{Experimental Design}\label{sec:design}

\section{Results}\label{sec:results}

\section{Discussion}\label{sec:discussion}

\subsection*{Replication}

The experiment performed herein can be duplicated with the Ruby-on-Rails code found in the github repository at \url{https://github.com/kapelner/neurotika_experiment}. The figures and tables in this manuscript can be replicated by running \texttt{paper\_duplication.R} found in the root of the repository. All code and scripts are open source under the MIT license.

\subsection*{Acknowledgements}

We thank Rikki Katz for help with software engineering for the experiment and Marie Le Pichon for the artwork used in the experiment.

\inputencoding{utf8}
\bibliographystyle{apalike}
\bibliography{mturk_refs,social_science_refs}

\appendix
\section{Demographic Questions}\label{app:demo_questions}
\section{Response Questions}\label{app:response_questions}

\end{document}

